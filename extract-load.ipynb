{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Entrega 1: Extract and Load**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instalar Pandas y Request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asegurarse de tener instaladas las librerías requests, pandas y deltalake. \n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importar librerías a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from deltalake import write_deltalake, DeltaTable\n",
    "from datetime import datetime\n",
    "from configparser import ConfigParser\n",
    "import os\n",
    "pd.options.display.max_rows = None # Visualizar todas las filas de los df\n",
    "pd.options.display.max_columns = None # Visualizar todas las columnas de los df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definir funciones y diccionario de ciudades a utilizar\n",
    "\n",
    "- Para mayor seguridad, las credenciales de la API se obtienen desde un archivo 'pipeline.conf'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para obtener datos desde la API\n",
    "def get_data(endpoint, cities):\n",
    "    # Leer archivo de configuración\n",
    "    config = ConfigParser()\n",
    "    config.read('pipeline.conf')\n",
    "\n",
    "    # Obtener valores desde el archivo de configuración\n",
    "    url_base = config['URL']['url_base']\n",
    "    api_key = config['API']['api_key']\n",
    "\n",
    "    # Construir solicitudes para cada ciudad\n",
    "    data_list = []\n",
    "    for city in cities:\n",
    "        url = f\"{url_base}/{endpoint}?lat={city['lat']}&lon={city['lon']}&appid={api_key}&units=metric\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            data_list.append(data)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error al obtener datos para {city['name']}: {e}\")\n",
    "            continue\n",
    "    return data_list\n",
    "\n",
    "# Función para construir el DataFrame a partir de los datos obtenidos\n",
    "def build_table(data_list, x=None):\n",
    "    try:\n",
    "        df = pd.json_normalize(data_list, record_path=x)\n",
    "        return df\n",
    "    except ValueError:\n",
    "        print(\"Los datos no están en el formato esperado\")\n",
    "        return None\n",
    "\n",
    "# Lista de ciudades con sus coordenadas\n",
    "cities = [\n",
    "    {'name': 'Córdoba', 'lat': -31.4135, 'lon': -64.1811},\n",
    "    {'name': 'Buenos Aires', 'lat': -34.6132, 'lon': -58.3772},\n",
    "    {'name': 'Rosario', 'lat': -32.9442, 'lon': -60.6505}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Current Weather - Endpoint de Datos Dinámicos - Extracción Incremental**\n",
    "\n",
    "API URL: https://openweathermap.org/current\n",
    "\n",
    "Con intervalos de una hora, se extraen datos del tiempo meteorológico actual. Esta extracción-carga se asegura mediante validaciones de guardar incrementalmente solo aquellos datos con fecha y hora posteriores a la de los datos existentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos de la última hora (20) agregados correctamente a la capa bronze.\n",
      "Cantidad total de registros en la bronze deltatable actualizada: 27\n"
     ]
    }
   ],
   "source": [
    "# Endpoint para el clima actual\n",
    "current_endpoint = 'weather'\n",
    "\n",
    "# Llamar a la función get_data\n",
    "current_data = get_data(current_endpoint, cities)\n",
    "\n",
    "# Construir el DataFrame aplanando JSON\n",
    "current_df = build_table(current_data)\n",
    "\n",
    "# Eliminar estas columnas innecesarias si es que existen\n",
    "# Se realiza esta transformación en esta etapa para mantener desde el inicio un schema acorde y así evitar inconsistencias ya que la API no suele devolver siempre la misma cantidad de columnas.\n",
    "columns_to_drop = ['weather', 'base', 'dt', 'timezone', 'cod', 'clouds.all', 'sys.type',\n",
    "                    'sys.id', 'sys.country', 'sys.sunrise', 'sys.sunset', 'wind.gust', 'rain.1h']                  \n",
    "current_df = current_df.drop(columns=[col for col in columns_to_drop if col in current_df.columns], axis=1, errors='ignore')\n",
    "\n",
    "# Agregar al dataframe columnas con fecha y hora actual\n",
    "current_df['date'] = datetime.now().strftime('%Y-%m-%d')\n",
    "current_df['hour_time'] = datetime.now().strftime('%H')\n",
    "\n",
    "# Directorio de la capa bronze\n",
    "bronze_dir_current = 'datalake/bronze/current_weather_data'\n",
    "\n",
    "# Validar si el directorio bronze existe\n",
    "if not os.path.exists(bronze_dir_current):\n",
    "    write_deltalake(\n",
    "        table_or_uri=bronze_dir_current,\n",
    "        data=current_df,\n",
    "        mode='append',\n",
    "        partition_by=['date', 'hour_time']\n",
    "    )\n",
    "    print(\"Delta Lake (bronze) inicializado con los datos actuales.\")\n",
    "else:\n",
    "    try:\n",
    "        # Obtener los datos existentes del bronze Delta Lake\n",
    "        existing_dt = DeltaTable(bronze_dir_current)\n",
    "        existing_data = existing_dt.to_pandas()\n",
    "\n",
    "        # Validar si ya existen datos para la misma combinación de date y hour_time\n",
    "        date_to_check = current_df['date'].iloc[0]\n",
    "        hour_to_check = current_df['hour_time'].iloc[0]\n",
    "\n",
    "        if ((existing_data['date'] == date_to_check) & (existing_data['hour_time'] == hour_to_check)).any():\n",
    "            print(f\"Ya existen datos en bronze para la fecha {date_to_check} y la hora {hour_to_check}. La inserción ha sido cancelada.\")\n",
    "        else:\n",
    "            # Si no hay duplicados, escribir los datos en el bronze Delta Lake\n",
    "            write_deltalake(\n",
    "                table_or_uri=bronze_dir_current,\n",
    "                data=current_df,\n",
    "                mode='append',  # Usa 'append' para realizar incrementos en cada extracción\n",
    "                partition_by=['date', 'hour_time']  # Columnas de partición\n",
    "            )\n",
    "            print(f\"Datos de la última hora ({hour_to_check}) agregados correctamente a la capa bronze.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar los datos en el bronze: {e}\")\n",
    "\n",
    "# Ver cantidad total de registros actuales de la bronze deltatable actualizada\n",
    "if os.path.exists(bronze_dir_current):\n",
    "    current_dt = DeltaTable(bronze_dir_current)\n",
    "    print(f\"Cantidad total de registros en la bronze deltatable actualizada: {current_dt.to_pandas().shape[0]}\")\n",
    "    \n",
    "# Descomentar las siguientes lineas para imprimir el último dataframe extraído-cargado\n",
    "# current_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. History Weather - Endpoint de Datos Estáticos - Extracción Full**\n",
    "\n",
    "API URL: https://openweathermap.org/forecast5\n",
    "\n",
    "La ejecución del siguiente código nos permitirá tener el pronótico de los próximos 5 días cada vez que ejecutemos el programa, sobreescribiendo los datos anteriores. La API arroja datos nuevos cada 3 horas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad total de registros en la bronze deltatable actualizada: 40\n"
     ]
    }
   ],
   "source": [
    "# API 5 day weather forecast - Pronóstico de clima para los próximos 5 días (data c/3hs)\n",
    "history_endpoint = 'forecast'\n",
    "\n",
    "# Obtener datos solo para Córdoba\n",
    "cordoba_data = [city for city in cities if city['name'] == 'Córdoba'] \n",
    "\n",
    "# Traer la data solo para Córdoba\n",
    "history_data = get_data(history_endpoint, cordoba_data)\n",
    "\n",
    "# Contruir tabla con datos json aplanados\n",
    "history_df = build_table(history_data, 'list')\n",
    "\n",
    "# URL directorio\n",
    "bronze_dir_history = 'datalake/bronze/history_weather_data'\n",
    "\n",
    "# Generar deltalake\n",
    "write_deltalake(\n",
    "    table_or_uri=bronze_dir_history,\n",
    "    data=history_df,\n",
    "    mode='overwrite',  # 'overwrite' para extracción full. Sobreescribe los datos antiguos\n",
    ")\n",
    "\n",
    "# Ver cantidad de registros en la tabla actualizada\n",
    "history_dt = DeltaTable(bronze_dir_history)\n",
    "print(f'Cantidad total de registros en la bronze deltatable actualizada: {history_dt.to_pandas().shape[0]}') # La cantidad se mantiene constante en 40 registros porque la data se sobreescribe\n",
    "\n",
    "# Descomentar la sigueinte línea para ver el último DataFrame extraído\n",
    "# history_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
